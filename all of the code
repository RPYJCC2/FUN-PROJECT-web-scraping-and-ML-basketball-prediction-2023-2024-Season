import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
import pandas as pd
import numpy as np
import random
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.preprocessing import StandardScaler
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split, KFold
import matplotlib.pyplot as plt

def daterange(start_date, end_date):
    for n in range(int((end_date - start_date).days) + 1):
        yield start_date + timedelta(days=n)
team_name_mapping = {
    "Cleveland": "CLE",
    "Charlotte": "CHO",
    "LA Lakers": "LAL",
    "New Orleans": "NOP",
    "Brooklyn": "BRK",
    "Oklahoma City": "OKC",
    "Milwaukee": "MIL",
    "Phoenix": "PHO",
    "New York": "NYK",
    "Atlanta": "ATL",
    "Boston": "BOS",
    "Chicago": "CHI",
    "Dallas": "DAL",
    "Denver": "DEN",
    "Detroit": "DET",
    "Golden State": "GSW",
    "Houston": "HOU",
    "Indiana": "IND",
    "LA Clippers": "LAC",
    "Memphis": "MEM",
    "Miami": "MIA",
    "Minnesota": "MIN",
    "Orlando": "ORL",
    "Philadelphia": "PHI",
    "Portland": "POR",
    "Sacramento": "SAC",
    "San Antonio": "SAS",
    "Toronto": "TOR",
    "Utah": "UTA",
    "Washington": "WAS",
}

def rate_limit(requests, per_minute, last_request_time):
    if len(requests) >= per_minute:
        time_since_oldest_request = datetime.now() - requests[0]
        if time_since_oldest_request < timedelta(minutes=1):
            sleep_time = (timedelta(minutes=1) - time_since_oldest_request).total_seconds() + 1
            print(f"Rate limit reached, sleeping for {sleep_time} seconds.")
            time.sleep(sleep_time)
        requests.pop(0)  # Remove the oldest request
    requests.append(datetime.now())

request_times = []

def scrape_basic_box_score_stats(url, team_abbr):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:86.0) Gecko/20100101 Firefox/86.0'
    }
    response = requests.get(url, headers=headers)
    rate_limit(request_times, 9, datetime.now())
    if response.status_code != 200:
        print(f"Failed to retrieve data from {url}")
        return {}
    
    soup = BeautifulSoup(response.content, 'html.parser')
    table_id = f'box-{team_abbr}-game-basic'
    table = soup.find('table', id=table_id)

    desired_stats = {
        'FGA': None,
        'FG_pct': None,
        'FTA': None,
        'FT_pct': None,
        'TRB': None,
        'fg3a': None,
        'fg3_pct': None,
        'TOV': None,
        'stl': None,
        'blk': None,
        'ast': None
    }

    if table:
        totals_row = table.find('tfoot').find('tr')
        if totals_row:
            for stat in desired_stats.keys():
                data_cell = totals_row.find('td', {'data-stat': stat.lower()})
                if data_cell:
                    desired_stats[stat] = data_cell.text.strip()

    return desired_stats

def scrape_general_game_info(date):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.190 Safari/537.36'
    }
    url = f"https://www.basketball-reference.com/boxscores/?month={date.month:02d}&day={date.day:02d}&year={date.year}"
    rate_limit(request_times, 9, datetime.now())  
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print(f"Request failed for {url} with status code {response.status_code}")
        return []
    
    soup = BeautifulSoup(response.content, 'html.parser')
    games_info = []

    game_summaries = soup.find_all('div', class_='game_summary expanded nohover')
    for game_summary in game_summaries:
        teams = game_summary.find_all('tr', class_=['winner', 'loser'])
        box_score_link_tag = game_summary.find('p', class_='links').find('a', text='Box Score')
        if len(teams) == 2 and box_score_link_tag:
            box_score_link = "https://www.basketball-reference.com" + box_score_link_tag['href']
            team1, team1_score = teams[0].find('a').text.strip(), teams[0].find('td', class_='right').text.strip()
            team2, team2_score = teams[1].find('a').text.strip(), teams[1].find('td', class_='right').text.strip()
            game_info = {
                'date': date.strftime('%Y-%m-%d'),
                'team1': team1,
                'team1_score': team1_score,
                'team2': team2,
                'team2_score': team2_score,
                'box_score_link': box_score_link
            }
            # Scrape basic box score stats for both teams
            #time.sleep(random.uniform(3, 6))
            game_info['team1_stats'] = scrape_basic_box_score_stats(box_score_link, team_name_mapping[team1])
            game_info['team2_stats'] = scrape_basic_box_score_stats(box_score_link, team_name_mapping[team2])
            games_info.append(game_info)
    return games_info






  # Define your date range
start_date = datetime(2023, 11, 1)
end_date = datetime(2024, 4, 5)
total_days = (end_date - start_date).days + 1

# Collect all games data
all_games_data = []
for single_date in tqdm(daterange(start_date, end_date), total=total_days, desc = "Scraping Data"):
    games_info = scrape_general_game_info(single_date)
    all_games_data.extend(games_info)

expanded_games_data = []

for game in all_games_data:
    # Extract basic game info
    game_info = {
        'date': game['date'],
        'team1': game['team1'],
        'team1_score': game['team1_score'],
        'team2': game['team2'],
        'team2_score': game['team2_score'],
    }

    # Extract team1 stats and prefix with 'team1_'
    team1_stats = {f'team1_{k}': v for k, v in game['team1_stats'].items()}
    game_info.update(team1_stats)

    # Extract team2 stats and prefix with 'team2_'
    team2_stats = {f'team2_{k}': v for k, v in game['team2_stats'].items()}
    game_info.update(team2_stats)

    # Add the expanded game info to the new list
    expanded_games_data.append(game_info)

# Convert to a DataFrame for analysis
df_games = pd.DataFrame(expanded_games_data)

# Display the dataset
print("Dataset with General Game Info and Basic Box Score Stats:")
print(df_games)



df_games.to_csv('br.csv', index = False)

br_df = pd.read_csv('br.csv')

unique_teams = pd.unique(br_df[['team1', 'team2']].values.ravel('K'))

# Map each team to a unique integer
team_to_id = {team: i for i, team in enumerate(unique_teams)}
print(team_to_id)
br_df['Home_Team_ID'] = br_df['team2'].map(team_to_id) # home team is team 2
br_df['Away_Team_ID'] = br_df['team1'].map(team_to_id) # away team is team 1

#Add win column
br_df['HomeWin'] = (br_df['team2_score'] > br_df['team1_score']).astype(int) #astype coverts boolean to integer 0 and 1

br_df['date'] = pd.to_datetime(br_df['date'])
br_df['month'] = (br_df['date'].dt.month - 11) % 12


print(br_df)



home_team = torch.tensor(br_df['Home_Team_ID'], dtype=torch.long)
away_team = torch.tensor(br_df['Away_Team_ID'], dtype=torch.long)
num_classes = 30 
home_teams_ohe = F.one_hot(home_team, num_classes=num_classes)
away_teams_ohe = F.one_hot(away_team, num_classes=num_classes)
date_stuff = torch.tensor(br_df['month'], dtype=torch.long)

months_ohe = F.one_hot(date_stuff, num_classes = 6)

x_ohe_tensor = torch.cat((home_teams_ohe, away_teams_ohe), dim=1)

numeric_data = br_df.drop(['date', 'Home_Team_ID', 'Away_Team_ID', 'HomeWin', 'team1', 'team2'], axis = 1).values
scaler = StandardScaler()
x_norm = scaler.fit_transform(numeric_data)
x_norm_tensor = torch.tensor(x_norm, dtype=torch.float32)

X_almost = torch.cat((x_norm_tensor, x_ohe_tensor), dim=1)
X = torch.cat((X_almost, months_ohe), dim = 1)

print(X.shape)

y = torch.tensor(br_df['HomeWin'], dtype=torch.long)



class Trainer:
    
    def __init__(self, model, opt_method, learning_rate, batch_size, epoch, l2):
        self.model = model
        if opt_method == "adam":
            self.optimizer = torch.optim.Adam(model.parameters(), learning_rate, weight_decay=l2)
        else:
            raise NotImplementedError("This optimization is not supported")
        
        self.epoch = epoch
        self.batch_size = batch_size
    
    def train(self, X_train, y_train, X_val, y_val, early_stop=True, draw_curve=True):
        train_dataset = TensorDataset(X_train, y_train)
        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        val_dataset = TensorDataset(X_val, y_val)
        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)
        
        train_loss_list, train_acc_list = [], []
        val_loss_list, val_acc_list = [], []
        weights = self.model.state_dict()
        lowest_val_loss = np.inf
        loss_func = nn.CrossEntropyLoss()
        for n in tqdm(range(self.epoch), leave=True):
            # enable train mode
            self.model.train()
            epoch_loss, epoch_acc = 0.0, 0.0
            for X_batch, y_batch in train_loader:
                batch_importance = y_batch.shape[0] / len(train_dataset)
                y_pred = self.model(X_batch)
                batch_loss = loss_func(y_pred, y_batch)
                
                self.optimizer.zero_grad()
                batch_loss.backward()
                self.optimizer.step()
                
                epoch_loss += batch_loss.detach().cpu().item() * batch_importance
                batch_acc = torch.sum(torch.argmax(y_pred, axis=1) == y_batch) / y_batch.shape[0]
                epoch_acc += batch_acc.detach().cpu().item() * batch_importance
            train_loss_list.append(epoch_loss)
            train_acc_list.append(epoch_acc)
            val_loss, val_acc = self.evaluate(val_dataset)
            val_loss_list.append(val_loss)
            val_acc_list.append(val_acc)
            
            if early_stop:
                if val_loss < lowest_val_loss:
                    lowest_val_loss = val_loss
                    weights = self.model.state_dict()
            
        if draw_curve:
            x_axis = np.arange(self.epoch)
            fig, axes = plt.subplots(1, 2, figsize=(10, 4))
            axes[0].plot(x_axis, train_loss_list, label="Train")
            axes[0].plot(x_axis, val_loss_list, label="Validation")
            axes[0].set_title("Loss")
            axes[0].legend()
            axes[1].plot(x_axis, train_acc_list, label='Train')
            axes[1].plot(x_axis, val_acc_list, label='Validation')
            axes[1].set_title("Accuracy")
            axes[1].legend()
            print(f"Validation accuracy: {np.mean(val_acc_list)}+/-{np.std(val_acc_list)}")
        
        if early_stop:
            self.model.load_state_dict(weights)
        
        return {
            "train_loss_list": train_loss_list,
            "train_acc_list": train_acc_list,
            "val_loss_list": val_loss_list,
            "val_acc_list": val_acc_list,
        }
    
    def evaluate(self, data, print_acc=False):
        # enable evaluation mode
        self.model.eval()
        loader = DataLoader(data, batch_size=self.batch_size, shuffle=True)
        loss_func = nn.CrossEntropyLoss()
        acc, loss = 0.0, 0.0
        for X_batch, y_batch in loader:
            with torch.no_grad():
                batch_importance = y_batch.shape[0] / len(data)
                y_pred = self.model(X_batch)
                batch_loss = loss_func(y_pred, y_batch)
                batch_acc = torch.sum(torch.argmax(y_pred, axis=1) == y_batch) / y_batch.shape[0]
                acc += batch_acc.detach().cpu().item() * batch_importance
                loss += batch_loss.detach().cpu().item() * batch_importance
        if print_acc:
            print(f"Accuracy: {acc:.3f}")
        return loss, acc




  def KFoldCrossValidation(
    model_class, k, 
    X, y,
    opt_method='adam', learning_rate=1e-3, batch_size=128, epoch=50, l2=0
):
    _, X_test, _, y_test = train_test_split(X, y, test_size=0.2)
    test_dataset = TensorDataset(X_test, y_test)
    kf = KFold(n_splits = k, shuffle = True)
    train_acc_list, test_acc_list = [], []
    for i, (train_index, val_index) in enumerate(kf.split(X)):
        print(f"Fold {i}:")
        
        X_train, X_val = X[train_index], X[val_index]
        y_train, y_val = y[train_index], y[val_index]
        
        model = model_class()
        # initialize a Trainer object
        trainer = Trainer(model, opt_method, learning_rate, batch_size, epoch, l2)
        # call trainer.train() here
        res = trainer.train(X_train, y_train, X_val, y_val)
        train_acc_best = res['train_acc_list'][np.argmin(res['val_loss_list'])]
        test_loss, test_acc = trainer.evaluate(test_dataset)
        if i == 1:
            torch.save(model.state_dict(), f'model_weights_fold_1.pth')
        train_acc_list.append(train_acc_best)
        test_acc_list.append(test_acc)
        
        print(f"Training accuracy: {train_acc_best}")
        print(f"Test accuracy: {test_acc}")
    
    print("Final results:")
    print(f"Training accuracy: {np.mean(train_acc_list)}+/-{np.std(train_acc_list)}")
    print(f"Test accuracy: {np.mean(test_acc_list)}+/-{np.std(test_acc_list)}")







  class WinPredictor(nn.Module):
    def __init__(self, ):
        super(WinPredictor, self).__init__()
        
        # The input dimension here accounts for both teams' embeddings plus the other features
        self.fc1 = nn.Linear(91, 128)
        
        self.fc2 = nn.Linear(128, 64)
        # Adjusting the output layer for binary classification (win/loss)
        self.output_layer = nn.Linear(64, 2) 

    def forward(self, x):
        
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        # Using softmax activation function
        win_prob = F.softmax(self.output_layer(x), dim = 1)
        
        return win_prob



  KFoldCrossValidation(WinPredictor, 3, X, y, batch_size = 32)



  # Initialize your model
model = WinPredictor()
model.load_state_dict(torch.load('model_weights_fold_1.pth'))  # Load trained model weights
model.eval()  # Set the model to evaluation mode

# Assuming there are 30 teams
num_classes = 30

team_names = {
  0: 'Washington',
  1: 'Indiana',
  2: 'Chicago',
  3: 'Portland',
  4: 'Sacramento',
  5: 'Charlotte',
  6: 'LA Clippers',
  7: 'Brooklyn',
  8: 'Denver',
  9: 'Cleveland',
  10: 'New Orleans',
  11: 'Milwaukee',
  12: 'Memphis',
  13: 'Detroit',
  14: 'Toronto',
  15: 'San Antonio',
  16: 'Orlando',
  17: 'Dallas',
  18: 'New York',
  19: 'Golden State',
  20: 'Boston',
  21: 'Utah',
  22: 'Atlanta',
  23: 'LA Lakers',
  24: 'Phoenix',
  25: 'Miami',
  26: 'Philadelphia',
  27: 'Oklahoma City',
  28: 'Minnesota',
  29: 'Houston'
}


# Example games with just home and away team IDs
games = [{"home_team_id": 23, "away_team_id": 19},
        {"home_team_id": 23, "away_team_id": 19},
        {"home_team_id": 19, "away_team_id": 23},
        {"home_team_id": 19, "away_team_id": 23},
        {"home_team_id": 23, "away_team_id": 19},
        {"home_team_id": 19, "away_team_id": 23},
        {"home_team_id": 23, "away_team_id": 19}
       ]


home_team_ids = torch.tensor([game["home_team_id"] for game in games])
away_team_ids = torch.tensor([game["away_team_id"] for game in games])

home_teams_ohe = F.one_hot(home_team_ids, num_classes=num_classes).float()
away_teams_ohe = F.one_hot(away_team_ids, num_classes=num_classes).float()

input_batch = torch.cat((home_teams_ohe, away_teams_ohe), dim=1)

# Pad the input batch to match the expected input size of the model
padding_size = 91 - input_batch.size(1)
padded_input_batch = F.pad(input_batch, (0, padding_size), "constant", 0)

# Make predictions for the batch
with torch.no_grad():
    logits = model(padded_input_batch)  # Raw outputs from the model
    probabilities = F.softmax(logits, dim=1)  # Convert logits to probabilities

# Interpret the predictions and print probabilities
for i, (prob, predicted_class) in enumerate(zip(probabilities, predicted_classes)):
    home_team_name = team_names[games[i]["home_team_id"]]
    away_team_name = team_names[games[i]["away_team_id"]]
    print(f"Game {i + 1} between {home_team_name} and {away_team_name}:")
    print(f"Probability of {home_team_name} winning: {prob[0].item():.2f}")
    print(f"Probability of {away_team_name} winning: {prob[1].item():.2f}")
    if predicted_class.item() == 0:
        print(f"Predicted winner: {home_team_name}\n")
    else:
        print(f"Predicted winner: {away_team_name}\n")
